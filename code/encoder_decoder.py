from rnn import *


class encoder_decoder(object):
    def __init__(self, enc_output_dim, enc_input_dim, enc_hidden_size, dec_output_dim, dec_input_dim, dec_hidden_size):
        '''
        Initialise parameters for the encoder and decoder
        '''
        self.encoder = RNN(enc_output_dim, enc_input_dim, enc_hidden_size)
        self.decoder = RNN(dec_output_dim, dec_input_dim, dec_hidden_size)

    def _minimize_loss(self, weights_derivatives_mem):
        '''
        Minimize the loss function by adagrad update
        '''
        for (param, dparam, mem) in weights_derivatives_mem:
            mem += dparam * dparam
            param += -self.learning_rate * dparam / np.sqrt(mem + 1e-8)

    def train(self, inputs, targets, num_epochs, vocab_to_ix, first_word_to_ix, model_q):
        '''
        Invoke BPTT for num_epochs
        '''
        enc_mWxh, enc_mWhh, enc_mWhy = np.zeros_like(self.encoder.Wxh), np.zeros_like(self.encoder.Whh), np.zeros_like(
        self.encoder.Why)
        enc_mbh, enc_mby = np.zeros_like(self.encoder.bh), np.zeros_like(self.encoder.by)  # memory variables for Adagrad
        dec_mWxh, dec_mWhh, dec_mWhy = np.zeros_like(self.decoder.Wxh), np.zeros_like(self.decoder.Whh), np.zeros_like(
            self.decoder.Why)
        dec_mbh, dec_mby = np.zeros_like(self.decoder.bh), np.zeros_like(self.decoder.by)  # memory variables for Adagrad
        dec_smooth_loss = -np.log(
            1.0 / self.decoder.output_dim) * self.decoder.input_dim  # loss at iteration 0 (not sure if it should be self.input_dim)
        enc_smooth_loss = -np.log(1.0 / self.encoder.output_dim) * self.encoder.input_dim
        for i in xrange(num_epochs):
            hprev = np.zeros((self.encoder.hidden_size, 1))  # reset RNN memory every epoch
            for j in xrange(len(inputs)):
                # Encoder forward pass
                enc_xs, enc_hs, enc_ps, enc_loss = self.encoder.BPTT([inputs[j]], [first_word_to_ix[targets[j].split(' ')[0]]], hprev, xs=None, hs=None, ps=None, forward=True)
                # smooth_loss = smooth_loss * 0.999 + loss * 0.001

                # Decoder Forward pass
		try:
                	dec_xs, dec_hs, dec_ps, dec_loss = self.decoder.BPTT([model_q[word] for word in targets[j].split(' ')[:-2]], [vocab_to_ix[word] for word in targets[j].split(' ')[1:]], enc_hs[len(inputs[j])-1], xs=None, hs=None, ps=None, forward=True)
		except:
			pdb.set_trace()
		# Decoder Back pass
                dec_loss, dec_dWxh, dec_dWhh, dec_dWhy, dec_dbh, dec_dby, dec_hprev = self.decoder.BPTT(
                    [model_q[word] for word in targets[j].split(' ')[:-2]],
                    [vocab_to_ix[word] for word in targets[j].split(' ')[1:]], dec_hs[len([model_q[word] for word in targets[j].split(' ')[:-2]])-1], xs=dec_xs, hs=dec_hs, ps=dec_ps, forward=False)

                # minimize the loss for deocder
                dec_weights_derivatives_mem = zip(
                    [self.decoder.Wxh, self.decoder.Whh, self.decoder.Why, self.decoder.bh, self.decoder.by],
                    [dec_dWxh, dec_dWhh, dec_dWhy, dec_dbh, dec_dby], [dec_mWxh, dec_mWhh, dec_mWhy, dec_mbh, dec_mby])
                self._minimize_loss(dec_weights_derivatives_mem)
                dec_smooth_loss = dec_smooth_loss * 0.999 + dec_loss * 0.001
                print 'epoch %d, DECODER loss: %f' % (i, dec_smooth_loss)  # print progress

                # Encoder Back pass
                enc_loss, enc_dWxh, enc_dWhh, enc_dWhy, enc_dbh, enc_dby, enc_hprev = self.encoder.BPTT([inputs[j]], [first_word_to_ix[targets[j].split(' ')[0]]], enc_hs[len(inputs[j])-1], xs=enc_xs, hs=enc_hs, ps=enc_ps, forward=False)

                # minimize the loss for deocder
                enc_weights_derivatives_mem = zip(
                    [self.encoder.Wxh, self.encoder.Whh, self.encoder.Why, self.encoder.bh, self.encoder.by],
                    [enc_dWxh, enc_dWhh, enc_dWhy, enc_dbh, enc_dby], [enc_mWxh, enc_mWhh, enc_mWhy, enc_mbh, enc_mby])
                self._minimize_loss(enc_weights_derivatives_mem)
                enc_smooth_loss = enc_smooth_loss * 0.999 + enc_loss * 0.001
                print 'epoch %d, ENCODER loss: %f' % (i, enc_smooth_loss)  # print progress


    def predict_question(self, X, ix_to_first_word):
        '''
        Given an input sequence, predict the output
        X : input sequence
        return output label/sequence
        '''
        ps, hs = self.encoder.predict(X)
        word = ix_to_first_word[ps.values().index(max(ps.values()))]
        model = pickle.load(open('ques.p'))
        while (word != '?'):
            print word, " ",
            ps, hs = self.decoder.predict(model[word], hs)
            word = ix_to_first_word[ps.values().index(max(ps.values()))]
        print word, "\n"
